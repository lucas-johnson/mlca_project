---
title: 'Building Footprint Identification with Airborne LiDAR: A Final Project for FOR 796'

author:
  - name: Lucas K Johnson
  
date: "`r Sys.Date()`"
header-includes:
    - \usepackage{float}
bibliography: report.bib
numbersections: true
output:
  bookdown::pdf_book:
    base_format: rticles::elsevier_article
    
    
keep_md: yes
abstract: |
  Airborne LiDAR has emerged as a uniquely valuable and information-rich source 
  of remote sensing (RS) data for high-resolution forest assessment and mapping.
  However, LiDAR data is limited in its ability to distinguish man-made 
  structures from natural ones, necessitating the addition of external forest
  masks in LiDAR-based forest mapping projects.
  These external masks are often either too inaccurate or too expensive, 
  leaving room for an efficient middle way. 
  In this report I aimed to produce cost-efficient models that can identify 
  buildings in 30m pixels. 
  I fit a simple logistic regression model and two machine-learning
  models with a set of 39 LiDAR-derived predictors and open source building
  footprint data. 
  Results indicated that both random forests and stochastic gradient boosting 
  machines can predict the presence of buildings with a high degree of accuracy
  (AUC = 0.96) given the input data used in this report. 
  With further assessment and tuning, these models can likely be used to 
  efficiently produce accurate forest masks anywhere high-quality airbone LiDAR 
  data has been acquired.

   

# note: non-breaking space (red highlight) above fixes line numbers in abstract
   
nocide: |
  @rmd_cook, @rmd_guide, @rmd_man, @ggplot2, @dplyr, @tidyselect, @kbl, @Hosking1990
---

```{r echo = FALSE, include = F}
library(kableExtra)
library(ggplot2)
library(here)
library(dplyr)
library(pROC)
library(tidyselect)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Introduction
Forest mapping and monitoring is becoming increasingly important as
federal, state, and global agencies look towards natural solutions
to mitigate a warming climate and the myriad resulting challenges. 
Field sampling programs, 
like the United States Department of Agriculture's Forest Inventory and Analysis
program (FIA) [@Gray2012],
provide unbiased estimates of forest structure over large areas, 
but lack the fine spatial resolution to understand and manage forests at 
relevant scales.
Thus, high-resolution forest mapping is needed to inform decision-makers where 
forest resources should be managed or preserved.

Airborne LiDAR has been established as the most valuable remote sensing data for
the purposes of forest structure mapping [@Huang2019; @Hurtt2019; @Chen2016].
However, due to the nature of this data and its near-singular ability to
characterize three-dimensional height-profiles, 
LiDAR cannot inherently distinguish between man-made structures and woody 
vegetation. 
To address this challenge, auxiliary landcover or forest canopy masks 
are often applied to LiDAR-modeled surfaces in attempt remove erroneous
predictions in buildings from those in forested areas [@Huang2019].
However, it is well documented that landcover maps are not 100% accurate, and
significant quantities of forest can be contained in non-forested classes 
[@Johnson2014; @Perry2008; @Meneguzzo2012].
Additionally, high-resolution tree-canopy delineation surfaces are expensive
to produce often relying on expert interpretation and iterative tuning [@Jarlath2013; @Jarlath2014].

In this report I attempt to find a middle way by producing models at reduced 
cost that can predict the presence of buildings in a mixed-use landscape with a high-degree of accuracy at a 30m resolution. 
To do this I train a simple logistic model, 
a random forest model, and a stochastic gradient boosting machine to classify 
the presence (1) or absence (0) of any buildings in a given map pixel.
Building indicator response data was derived from an open-source building 
footprint dataset developed by Microsoft [@Microsoft; @Bing].
The predictor data used to train these models are LiDAR-derived grid metrics, commonly used in models of forest-structure, aiding in the reduction of cost. 
If these models prove to be successful classifiers of building presence, 
the same predictors will afford future forest-structure modelers double the 
benefit.

# Methods

## Building and LiDAR Data

Rasterized building footprint data served as the response data in this study 
[@Heris2020a; @Heris2020b; @Bing; @Microsoft].
The raw raster contains counts of the number of buildings intersecting each 
pixel. 
This data was converted to a Boolean raster where 1s represent the presence of
any buildings, and 0s represent a complete lack of buildings. 
This data was chosen for its availability across the entire country, its high
accuracy (> 99% positive predictive value), and its 30m resolution
[@Heris2020b].

The raw LiDAR data originates from a single acquisition covering the city
of Buffalo and larger portions of Erie, Genesee, and Livingston counties in 
western New York [@EGL_Lidar]. 
This particular data was selected due to its known ability to characterize 
three dimensional height-profiles at high-resolution, and the range of 
landcover conditions (urban, forest, cropland).
The data was made available by the New York State GIS Program Office. 
The raw LiDAR data was height-normalized and converted into a set of 39 
predictors (Table \@ref(tab:predictors)) chosen for their prevalence in models 
of forest structure [@Hawbaker2010; @Huang2019; @Pflugmacher2014]. 

The LiDAR predictors, in raster stack form, were overlaid with the Boolean
building raster to create stack of data where each pixel contained a set of 39 
predictors and one building indicator response variable.
A stratified random sample was conducted on the raster stack, with the building
indicator providing the levels of stratification. 
3,500 pixels were selected from each stratum resulting in 7,000 observations for model training and testing. 
This final dataset was converted to a 7000x40 (rows, columns) data frame.
The lidR [@lidrCRAN; @lidrRSE] and raster [@Raster2021] packages were used for 
height-normalization and dataset generation.
Additionally, the first seven principle components were derived from the final dataset to produce an alternative dataset without multicollinearity predictor
dataset [@RCore]. 
This alternative dataset accounted for $\ge$ 95% of the information in the raw predictors and existed as a 7000x7 data frame.


```{r predictors, echo = FALSE, message = FALSE, warning = FALSE}
dplyr::tribble(
  ~ "Predictor", ~ "Definition", ~ "Group",
  "H0, H10, ... H100, H95, H99", "Decile heights of returns, in meters, as well as 95th and 99th percentile return heights.", "LiDAR",
  "D10, D20... D90", "Density of returns above a certain height, as a proportion. After return height is divided into 10 equal bins ranging from 0 to the maximum height of returns, this value reflects the proportion of returns at or above each breakpoint.",  "LiDAR",
  "ZMEAN, ZMEAN_C", "Mean height of all returns (ZMEAN) and all returns above 2.5m (ZMEAN_C)",  "LiDAR",
  "Z_KURT, Z_SKEW", "Kurtosis and skewness of height of all returns",  "LiDAR",
  "QUAD_MEAN, QUAD_MEAN_C", "Quadratic mean height of all returns (QUAD_MEAN) and all returns above 2.5m (QUAD_MEAN_C)",  "LiDAR",
  "CV, CV_C", "Coefficient of variation for heights of all returns (CV) and all returns above 2.5m (CV_C)",  "LiDAR",
  "L2, L3, L4, L_CV, L_SKEW, L_KURT", "L-moments and their ratios as defined by Hosking (1990), calculated for heights of all returns",  "LiDAR",
  "CANCOV", "Ratio of returns above 2.5m to all returns (Pflugmacher et al. 2012)",  "LiDAR",
  "HVOL", "CANCOV * ZMEAN (Pflugmacher et al. 2012)",  "LiDAR",
  "RPC1", "Ratio of first returns to all returns (Pflugmacher et al. 2012)",  "LiDAR"
) %>%
  select(-Group) %>%
  kbl(booktabs = TRUE, align = "l",
      caption = "Definitions of predictors used for model fitting.",
      linesep = "\\addlinespace") %>% 
  row_spec(0, align = "c") %>% 
  column_spec(1, width = "10em") %>% 
  column_spec(2, width = "22em")
```

## Models

Three candidate classification models were fit to a random 70% 
(calibration data; n = 
`r nrow(read.csv(here("data/training.csv")))`
)
of the observations, with the remaining 30% reserved for model performance 
assessment (holdout data; n = 
`r nrow(read.csv(here("data/testing.csv")))`
).
The first candidate model was a simple logistic regression model [@RCore], 
and was trained on the principle components variant of the calibration data.
The second candidate model was a random forest (RF herafter) trained with
the ranger R package [@Wright2017]. 
The third candidate model was a stochastic gradient boosting machine 
(LGB hereafter) trained with the LightGBM R package [@Guolin2021]. 
The hyperparemeters for both the RF and LGB models were selected using a 
standard grid search where each combination of hyperparameters were compared 
against eachother using the cross-entropy loss function 
(CEL; Equation \@ref(eq:cel))
computed from a random five-fold cross-validation with the calibration dataset.
CEL is computed as follows:

\begin{equation}
\operatorname{CEL} = \sum_{i=1}^{n}{-\log{(\hat{y_{i}})}} (\#eq:cel)
\end{equation}

Where $n$ is the number of observations in the fold, and $\hat{y_i}$ is 
the predicted probability of the true class.

Postitive prediction thresholds for all models were chosen using the optimal
ROC coordinates for the fully tuned models fit to the calibration data.
Each of the models were assessed against the holdout dataset and compared to one
another using overall accuracy, specificity, sensitivity, and AUC. 
Additionally ROC curves were plotted for each model's results on the holdout 
set. 
The caret and pROC R packages were used to compute these accuracy metrics 
[@caret; @pROC].

# Results
The RF and LGB models were significantly better than the Logistic model 
across all accuracy metrics (Table \@ref(tab:metrics)). 
While the RF and LGB models shared the same AUC and Overall Accuracy results, 
the RF model was sligthly more specific while the LGB model was slightly more sensitive.
However, all three candidate models performed quite well with 
all AUC values $\geq$ 0.87, and all overall accuracies $\geq$ 0.79. 
The ROC curves plotted in Figure \@ref(fig:roc) display similar patterns. 

```{r metrics}
get_metric_row <- function(summary_obj, model_name) {
  list(
    model = model_name,
    auc = summary_obj$auc[[1]],
    overall = summary_obj$confusion$overall[[1]],
    sensitivity = summary_obj$confusion$byClass[[1]],
    specificity = summary_obj$confusion$byClass[[2]]
  )
}
bind_rows(
  get_metric_row(readRDS(here("data/logistic_summary.rds")), "Logistic"),
  get_metric_row(readRDS(here("data/rf_summary.rds")), "RF"),
  get_metric_row(readRDS(here("data/lgb_summary.rds")), "LGB")
) |>
  mutate(across(where(is.numeric), round, 2)) |>
  arrange(auc) |>
  kbl(
    col.names = c(
      "Model", "AUC", "Overall Accuracy", "Sensitivity", "Specificity"
    ),
    booktabs = TRUE,
    align = c("l", rep("r", 4)), 
    linesep = "\\addlinespace",
    caption = gsub(
      "%",
      "\\\\%",
      sprintf(
        "Model accuracy metrics computed against holdout partition (n = %i).",
        nrow(read.csv(here("data/testing.csv")))
      )
    )
  )|>
  row_spec(0, align = "c") |>
  kable_styling(font_size = 12)
```

```{r roc, fig.cap = sprintf("ROC curves for three models tested against the holdout partition (n = %i).", nrow(read.csv(here("data/testing.csv")))), out.width = "100%"}
logistic_summary <- readRDS(here("data/logistic_summary.rds"))
rf_summary <- readRDS(here("data/rf_summary.rds"))
lgb_summary <- readRDS(here("data/lgb_summary.rds"))

ggroc(
  list(
    "LGB" = lgb_summary$roc, 
    "RF" = rf_summary$roc, 
    "Logistic" = logistic_summary$roc
  ),
  size = 1,
  legacy.axes = TRUE
) + 
  scale_y_continuous(expand = expansion(c(0.005, 0.07))) +
  scale_x_continuous(expand = expansion(c(0.005, 0.07))) +
  scale_colour_brewer(palette = "Dark2") +
  theme_minimal() + 
  labs(color='Model', x = "False Positive Rate", y = "Sensitivity") +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.text = element_text(size = 10), 
    legend.title = element_text(size = 12),
    axis.line.y = element_line(size = 0.5),
    axis.line.x = element_line(size = 0.5)
  )
```


# Discussion
It is unsurprising that the two machine learning models (RF and LGB)
outperformed the simple Logistic regression model given the constraints
on multicollinearity required for Logistic regression. 
In particular, the Logistic model might have been improved by including more
principle components, as in this case we limited the input data to the first
seven principle components which accounted for 95% of the information in the
raw predictors. 
This may have been an unfair disadvantage as the RF and LGB models were given
the opportunity to leverage 100% of the information in the predictor space.

There are a few ways I could further improve the models described in this 
report. 
First, more extensive grid searches could have been performed to find better
hyperparameters. 
There is a trade-off here between the combination of time 
and performance gains, with eventually diminishing returns on the time invested.
The relatively limited tuning performed in this report produced models that were
good enough for my purposes. 
Additionally, I might be able to produce an even better model by using 
stacked ensembles, which often serve to reduce predictive error, especially
when the error from component models is dominated by variance [@Dormann2018]. 
Noisy data, which we can assume categorizes our LiDAR and building data, often
yields models with variance dominated error [@Dormann2018].
One potential source of error in our models is the temporal match between 
predictor and response data used to fit the models. 
The building data, though published in 2018, has no associated 
time-of-acquisition requiring us to hope that the building classifications
describe conditions close to those represented in the 2019 LiDAR acquisition
[@Heris2020b].

Since the predictors used to train these models are primarily used for 
predicting forest structure, these models provide a relatively simple option
to mask man-made structures from LiDAR-derived maps of forest structure like 
aboveground biomass or canopy height. 
Further investigation is required to assess the transferrability of these models
trained in one region with one LiDAR acquisition to others.
If separate models are required for each distinct LiDAR acquisition or region, 
the relative benefit of the models developed in this report would only be 
slightly diminsihed, as these models would still serve as a reference point for 
other applications. 
Finally, a true accuracy assessment, using time-relevant reference data of 
higher quality than the building data used herein should be conducted to assess 
the suitability of these models in real-world applications [@Stehman2019].

\newpage{}

# References {#references .unnumbered}
